<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="stylesheet/jemdoc.css" type="text/css" />
<link rel="stylesheet" href="stylesheet/styles.css" type="text/css" />
<style>
table, th, td {
  border: 1px solid black;
  border-collapse: collapse;
}
th, td {
  padding: 5px;
}
th {
  text-align: left;
}
</style>

<title>Guangneng HU</title>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<link rel="stylesheet" href="fontawesome-free-6.0.0-beta2-web/css/fontawesome.min.css">
</head>

<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">

<td id="layout-content">
<div id="toptitle">
<div class="call-out-green"><div style="font-size:36px; padding:13px 0px; font-family:Arial, Helvetica, sans-serif; font-weight:bold;";>Multimodal Learning Tasks [<i><a href="index.html">BackToHome</a></i>]</div></div>
</div>

<h2 style="CLEAR: both;">Multimodal NER</h2>
<ol reversed>  
<li><a href="paper/Multimodal Named Entity Recognition with Knowledge-Refined Cross-Modal Attention-wang-icme22.pdf" target="_blank">CAT-MNER: Multimodal Named Entity Recognition with Knowledge-Refined Cross-Modal Attention</a>. Wang et al., <i>ICME</i>, 2022

<li><a href="paper/A General Matching and Alignment Framework for Multimodal Named Entity Recognition-Xu-wsdm22.pdf" target="_blank">MAF: A General Matching and Alignment Framework for Multimodal Named Entity Recognition</a>. Xu et al. <i>WSDM</i>, 2022 [<a href="https://github.com/xubodhu/MAF" target="_blank">MAF-code</a>]
<br><i>two auxiliary tasks (self-supervised learning matching+alignment)</i>

<li><a href="paper/Pretraining Multi-modal Representations for Chinese NER Task with Cross-Modality Attention-Mai-wsdm22.pdf" target="_blank">Pretraining Multi-modal Representations for Chinese NER Task with Cross-Modality Attention</a>. Mai et al. <i>WSDM</i>, 2022

<li><a href="paper/ITA-Image-Text Alignments for Multi-Modal Named Entity Recognition-Wang-naacl22.pdf" target="_blank">ITA: Image-Text Alignments for Multi-Modal Named Entity Recognition</a>. Wang et al. <i>NAACL</i>, 2022 [<a href="https://github.com/Alibaba-NLP/KB-NER/tree/main/ITA" target="_blank">ITA-code</a>]
<br><i>align image into object tags (local), captions (global) and OCR as visual contexts; KL(cross-modal view,text view)</i>

<li><a href="paper/Multi-modal Graph Fusion for Named Entity Recognition with Targeted Visual Guidance-Zhang-aaai21.pdf" target="_blank">Multi-modal Graph Fusion for Named Entity Recognition with Targeted Visual Guidance</a>. Zhang et al. <i>AAAI</i>, 2021 [<a href="https://github.com/TransformersWsz/UMGF" target="_blank">UMGF-code</a>]
<br><i>multimodal GNNs for NER; averagely segemented vs. targeted visual cues; cross-domain generalizatoin comparison</i>

<li><a href="paper/RpBERT A text-image relation propagation-based BERT model for multimodal NER-Sun-aaai21.pdf" target="_blank">RpBERT: A text-image relation propagation-based BERT model for multimodal NER</a>. Sun et al. <i>AAAI</i>, 2021
<br><i>multimodal BERT for NER; one auxiliary task (text-image relation [CLS]) on the external source (the <a href="paper/Categorizing and Inferring the Relationship between the Text and Image of Twitter Posts-Vempala-acl19.pdf" target="_blank">TRC</a> data)</i>

<li><a href="paper/RIVA A Pre-trained Tweet Multimodal Model Based on Text-image Relation for Multimodal NER-Sun-COLING20.pdf" target="_blank">RIVA: A Pre-trained Tweet Multimodal Model Based on Text-image Relation for Multimodal NER</a>. Sun et al. <i>COLING</i>, 2020
<br><i>Relationship Inference and Visual Attention (RIVA); auxiliary task (text-image relation [CLS]) on unlabeled
large tweet corpus</i>

<li><a href="paper/Multimodal Representation with Embedded Visual Guiding Objects for Named Entity Recognition in Social Media Posts-Wu-mm20.pdf" target="_blank">Multimodal Representation with Embedded Visual Guiding Objects for Named Entity Recognition in Social Media Posts</a>. Wu et al. <i>ACM MM</i>, 2020
<br><i>OCSGA: Object + Character + SA (Self-Attention) + GA (Guide Attention)</i>

<li><a href="paper/Improving Multimodal Named Entity Recognition via Entity Span Detection with Unified Multimodal Transformer-Yu-acl20.pdf" target="_blank">Improving Multimodal Named Entity Recognition via Entity Span Detection with Unified Multimodal Transformer</a>. Yu et al. <i>ACL</i>, 2020 [<a href="https://github.com/jefferyYu/UMT" target="_blank">UMT-code</a>]
<br><i>multimodal Transformer for NER; one auxiliary task (entity span detection)</i>

<li><a href="paper/Visual Attention Model for Name Tagging in Multimodal Social Media-Lu-acl18.pdf" target="_blank">Visual Attention Model for Name Tagging in Multimodal Social Media</a>. Lu et al. <i>ACL</i>, 2018
<br><i>Twitter-17 & Snapchat datasets; multimodal Hierarchy BiLSTM-CRF for NER</i>

<li><a href="paper/Multimodal Named Entity Recognition for Short Social Media Posts-Moon-naacl18.pdf" target="_blank">Multimodal Named Entity Recognition for Short Social Media Posts</a>. Moon et al. <i>NAACL</i>, 2018
<br><i>SnapCaptions dataset</i>

<li><a href="paper/Adaptive Co-attention Network for Named Entity Recognition in Tweets-Zhang-aaai18.pdf" target="_blank">Adaptive Co-attention Network for Named Entity Recognition in Tweets</a>. Zhang et al. <i>AAAI</i>, 2018
<br><i>Twitter-15 dataset; multimodal CNN-BiLSTM-CRF for NER</i>
</ol> 

<strong>Survey & Journal</strong> 
<ol reversed>  
<li><a href="paper/A Survey on Deep Learning for Named Entity Recognition-Li-tkde20.pdf" target="_blank">A Survey on Deep Learning for Named Entity Recognition</a>. Li et al. <i>TKDE</i>, 2020
<li><a href="paper/Object-Aware Multimodal Named Entity Recognition in Social Media Posts With Adversarial Learning-Zhang-TMM21.pdf" target="_blank">Object-Aware Multimodal Named Entity Recognition in Social Media Posts With Adversarial Learning</a>. Zhang et al. <i>TMM</i>, 2021
</ol> 

<strong>Neural & Transfer/ Cross-domain NER</strong> 
<ol reversed>  
<li><a href="paper/Exploring Modular Task Decomposition in Cross-domain Named Entity Recognition-Zhang-sgir22.pdf" target="_blank">Exploring Modular Task Decomposition in Cross-domain Named Entity Recognition</a>. Zhang et al. <i>SIGIR</i>, 2022 [<a href="https://github.com/AIRobotZhang/MTD" target="_blank">MTD-code</a>]
<li><a href="paper/Effective Named Entity Recognition with Boundary-aware Bidirectional Neural Networks-Li-www22.pdf" target="_blank">Effective Named Entity Recognition with Boundary-aware Bidirectional Neural Networks</a>. Li et al. <i>TheWebConf</i>, 2022
<li><a href="paper/End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF-Ma-acl16.pdf" target="_blank">End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF</a>. Ma & Hovy. <i>ACL</i>, 2016
</ol> 

<strong>Workshop</strong> 
<ol reversed>  
<li><a href="paper/Can images help recognize entities A study of the role of images for Multimodal NER-Chen-emnlp21ws.pdf" target="_blank">Can images help recognize entities: A study of the role of images for Multimodal NER</a>. Chen et al. <i>EMNLP workshop</i>, 2021
</ol> 

<h2 style="CLEAR: both;">Multimodal MT</h2>
<ol reversed>  
<li><a href="paper/A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation-Yin-acl20.pdf" target="_blank">A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation</a>. Yin et al. <i>ACL</i>, 2020
<li><a href="paper/A Shared Task on Multimodal Machine Translation and Crosslingual Image Description-Specia-WMT16.pdf" target="_blank">A Shared Task on Multimodal Machine Translation and Crosslingual Image Description</a>. Specia et al. <i>WMT</i>, 2016
</ol> 

<h2 style="CLEAR: both;">Multimodal IE</h2>
<ol reversed>  
<li><a href="paper/Modeling Dense Cross-Modal Interactions for Joint Entity-Relation Extraction-Zhao-ijcai20.pdf" target="_blank">Modeling Dense Cross-Modal Interactions for Joint Entity-Relation Extraction</a>. Zhao et al. <i>IJCAI</i>, 2020
</ol> 

<h2 style="CLEAR: both;">Multimodal KG</h2>

<h2 style="CLEAR: both;">Multimodal Sentiment/ Emotion</h2>
<ol reversed> 
<li><a href="paper/Multimodal Transformer for Unaligned Multimodal Language Sequences-Tsai-acl19.pdf" target="_blank">Multimodal Transformer for Unaligned Multimodal Language Sequences</a>. Tsai et al. <i>ACL</i>, 2019
<br><i>crossmodal attention, crossmodal Transformer, multimodal Transformer</i>

<li><a href="paper/Adapting BERT for Target-Oriented Multimodal Sentiment Classification-Yu-ijcai19.pdf" target="_blank">Adapting BERT for Target-Oriented Multimodal Sentiment Classification</a>. Yu & Jiang. <i>IJCAI</i>, 2019
</ol> 

<h2 style="CLEAR: both;">Vision+Language (VQA, NLVR, VCSR, I-T Retri., V Entail., REC)</h2>
<ol reversed>  
<li><a href="paper/UNITER UNiversal Image-TExt Representation Learning-Chen-eccv20.pdf" target="_blank">UNITER: UNiversal Image-TExt Representation Learning</a>. Chen et al. <i>ECCV</i>, 2020
<br><i>Optimal Transport for finegrained alignment between words and image regions</i>

<li><a href="paper/Oscar Object-Semantics Aligned Pre-training for Vision-Language Tasks-Li-eccv20.pdf" target="_blank">Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks</a>. Li et al. <i>ECCV</i>, 2020
<br><i>object tags as anchor points to learn alignment</i>

<li><a href="paper/VL-BERT Pre-training of Generic Visual-Linguistic Representations-Su-iclr20.pdf" target="_blank">VL-BERT: Pre-training of Generic Visual-Linguistic Representations</a>. Su et al. <i>ICLR</i>, 2020

<li><a href="paper/Unicoder-VL A Universal Encoder for Vision and Language by Cross-Modal Pre-Training-Li-aaai20.pdf" target="_blank">Unicoder-VL: A Universal Encoder for Vision and Language by Cross-Modal Pre-Training</a>. Li et al. <i>AAAI</i>, 2020

<li><a href="paper/ViLBERT Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks-Lu-neurips19.pdf" target="_blank">ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks</a>. Lu et al. <i>NeurIPS</i>, 2019

<li><a href="paper/Deep Modular Co-Attention Networks for Visual Question Answering-Yu-cvpr19.pdf" target="_blank">Deep Modular Co-Attention Networks for Visual Question Answering</a>. Yu et al. <i>CVPR</i>, 2019
<br><i>Self Attn (word-to-word, region-to-region); Guided Attn (word-to-region)</i>

<li><a href="paper/LXMERT Learning Cross-Modality Encoder Representations from Transformers-Tan-Bansal-emnlp19.pdf" target="_blank">LXMERT: Learning Cross-Modality Encoder Representations from Transformers</a>. Tan & Bansal. <i>EMNLP</i>, 2019
<br><i>three endoers: object relationship, language,
and cross-modality</i>

<li><a href="paper/VisualBERT A Simple and Performant Baseline for Vision and Language-Harold Li-Harold-arxiv19.pdf" target="_blank">VisualBERT: A Simple and Performant Baseline for Vision and Language</a>. Harold et al. <i>arXiv</i>, 2019
</ol> 



<p> [<i><a href="index.html">BackToHome</a></i>]

</td>
</tr>
</table>
</body>
</html>